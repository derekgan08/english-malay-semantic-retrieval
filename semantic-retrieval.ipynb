{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CPC353 Assignment 2**\n",
    "\n",
    "## **This assignment is to classifying English-Malay sentences according to their semantics**\n",
    "\n",
    "I used the GloVe 6B 100d pre-trained dwonloaded from [kaggle](https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt?resource=download) as the pre-trained embedding to embed the word vocabularies from the datasets given:\n",
    "\n",
    "\n",
    "1.   `train.en` - training files of English Sentences\n",
    "2.   `train.my` - training files of Malay Sentences\n",
    "3. `train.cl` - training files of the classes\n",
    "4. `test.en` - testing files of English Sentences\n",
    "5. `test.my` - testing files of Malay Sentences\n",
    "6. `test.cl` - testing files of the classes\n",
    "7. `dev.en` - validating files of English Sentences\n",
    "8. `dev.my` - validating files of Malay Sentences\n",
    "9. `dev.cl` - validating files of the classes\n",
    "\n",
    "There are 3 files for each training, testing and validation sets. The .en files are english sentences, the .my files are malay sentences, and the .cl files are the labels for english and malay files to indicate whether are both sentences similar. The .cl files are just 1s and 0s.\n",
    "\n",
    "The notebook is sectioned into multiple steps: Tokenizing text, embedding text, building neural network, and testing the network.\n",
    "\n",
    "### Neural network architecture\n",
    "The neural network architecture starts with two models. One for the English language, one for the Malay language. After that, I will concatenate both models into one layer where it will perform binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Reading the Text Files and Tokenizing the Texts**\n",
    "\n",
    "I will first read the files into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(text_path):\n",
    "    # This function will read the txt file and split the sentences with new line character, and will also remove the\n",
    "    # last character since it is an empty string\n",
    "    return open(text_path, \"r\", encoding=\"utf8\").read().split(\"\\n\")[:-1]\n",
    "\n",
    "train_text_en = read_text(\"/kaggle/input/english-malay/train.en\")\n",
    "train_text_my = read_text(\"/kaggle/input/english-malay/train.my\")\n",
    "train_label = read_text(\"/kaggle/input/english-malay/train.cl\")\n",
    "\n",
    "test_text_en = read_text(\"/kaggle/input/english-malay/test.en\")\n",
    "test_text_my = read_text(\"/kaggle/input/english-malay/test.my\")\n",
    "test_label = read_text(\"/kaggle/input/english-malay/test.cl\")\n",
    "\n",
    "dev_text_en = read_text(\"/kaggle/input/english-malay/dev.en\")\n",
    "dev_text_my = read_text(\"/kaggle/input/english-malay/dev.my\")\n",
    "dev_label = read_text(\"/kaggle/input/english-malay/dev.cl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining all imports for the project\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Input, Concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of text will be carried using the Tokenizer class from keras. Since I will be concatenating two models for two langauges. That means that I will have to tokenize both english and malay sentences separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "en_tokenizer = Tokenizer()\n",
    "en_tokenizer.fit_on_texts(train_text_en)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(train_text_en)\n",
    "\n",
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.fit_on_texts(train_text_my)\n",
    "my_sequences = my_tokenizer.texts_to_sequences(train_text_my)\n",
    "\n",
    "# Find the maximum length between the two sequences, so we can pad them\n",
    "max_length = max(len(max(en_sequences, key=len)), len(max(my_sequences, key=len)))\n",
    "\n",
    "# Padding the sequences to same length\n",
    "en_sequences = pad_sequences(en_sequences, maxlen=max_length)\n",
    "my_sequences = pad_sequences(my_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in English text data:  39908\n",
      "Number of unique words in Malay text data:  23279\n"
     ]
    }
   ],
   "source": [
    "# Showing the number of vocabulary in both languages training data\n",
    "# Uncomment for the tokens and sequences\n",
    "\n",
    "print(\"Number of unique words in English text data: \", len(en_tokenizer.word_index))\n",
    "# print(\"English text data is: \", en_tokenizer.word_index)\n",
    "# print(\"English seqeunce: \", en_sequences)\n",
    "\n",
    "print(\"Number of unique words in Malay text data: \", len(my_tokenizer.word_index))\n",
    "# print(\"Malay text data is: \", my_tokenizer.word_index)\n",
    "# print(\"Malay seqeunce: \", my_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Embedding the Vocabulary**\n",
    "\n",
    "The next step we will get our word embeddings of our dataset based on the pre-trained GloVe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper function to embed the vocabularies\n",
    "def vocab_embedding(filepath, word_index, embedding_dim):\n",
    "  # Using the tokenizer word index attribute, we can determine the vocabulary size\n",
    "  vocab_size = len(word_index) + 1\n",
    "  embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "  with open(filepath) as f:\n",
    "    for line in f:\n",
    "      word, *vector = line.split()\n",
    "      if word in word_index:\n",
    "        idx = word_index[word]\n",
    "        embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float)[:embedding_dim]\n",
    "\n",
    "  return embedding_matrix_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "en_embedding_matrix_vocab = vocab_embedding(\"/kaggle/input/glovedata/glove.6B.100d.txt\", en_tokenizer.word_index, 100)\n",
    "my_embedding_matrix_vocab = vocab_embedding(\"/kaggle/input/glovedata/glove.6B.100d.txt\", my_tokenizer.word_index, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Building the Neural Network**\n",
    "\n",
    "I will be using a Long-Short Term Memory (LSTM) neural network for both languages.\n",
    "\n",
    "Both LSTM neural network starts with an Input layer followed by an Embedding layer. After embedding, we will pass the network into a Dropout layer to avoid overfitting. After the network passes through the embedding layer, it will then enter the LSTM layer.\n",
    "\n",
    "After the LSTM layer, both model for both languages will then be concatenated into two Dense layer. One with a activation function of Rectified Linear Unit (ReLU) and the second one, which is also the output layer with an activation function of Sigmoid for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_input = Input(shape=(en_sequences.shape[1],))\n",
    "malay_input = Input(shape=(my_sequences.shape[1],))\n",
    "\n",
    "en_num_words = len(en_tokenizer.word_index) + 1\n",
    "my_num_words = len(my_tokenizer.word_index) + 1\n",
    "\n",
    "# Passing the inputs to the embedding layer\n",
    "embedded_english = Embedding(\n",
    "    input_dim = en_num_words,\n",
    "    output_dim = 100,\n",
    "    input_length = len(train_text_en),\n",
    "    weights=[en_embedding_matrix_vocab]\n",
    ")(english_input)\n",
    "\n",
    "embedded_malay = Embedding(\n",
    "    input_dim = my_num_words,\n",
    "    output_dim = 100,\n",
    "    input_length = len(train_text_my),\n",
    "    weights=[my_embedding_matrix_vocab]\n",
    ")(malay_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dropout = Dropout(rate=0.2)(embedded_english)\n",
    "my_dropout = Dropout(rate=0.2)(embedded_malay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass embedded inputs to a LSTM layer\n",
    "lstm = LSTM(100)\n",
    "english_output = lstm(en_dropout)\n",
    "malay_output = lstm(my_dropout)\n",
    "\n",
    "lstm_concatenated = Concatenate()([english_output, malay_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = Dense(64, activation='relu')(lstm_concatenated)\n",
    "output = Dense(1, activation='sigmoid')(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[english_input, malay_input], outputs=output)\n",
    "\n",
    "# Compile the whole model.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_callback = [\n",
    "    ModelCheckpoint(\"/kaggle/working/best_model\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3750/3750 [==============================] - 122s 32ms/step - loss: 0.3646 - accuracy: 0.8317 - val_loss: 0.3194 - val_accuracy: 0.8633\n",
      "Epoch 2/10\n",
      "3750/3750 [==============================] - 119s 32ms/step - loss: 0.1873 - accuracy: 0.9283 - val_loss: 0.3343 - val_accuracy: 0.8647\n",
      "Epoch 3/10\n",
      "3750/3750 [==============================] - 118s 31ms/step - loss: 0.1142 - accuracy: 0.9591 - val_loss: 0.3691 - val_accuracy: 0.8630\n",
      "Epoch 4/10\n",
      "3750/3750 [==============================] - 116s 31ms/step - loss: 0.0740 - accuracy: 0.9733 - val_loss: 0.3775 - val_accuracy: 0.8764\n",
      "Epoch 5/10\n",
      "3750/3750 [==============================] - 117s 31ms/step - loss: 0.0481 - accuracy: 0.9829 - val_loss: 0.4363 - val_accuracy: 0.8748\n",
      "Epoch 6/10\n",
      "3750/3750 [==============================] - 116s 31ms/step - loss: 0.0327 - accuracy: 0.9889 - val_loss: 0.6055 - val_accuracy: 0.8570\n",
      "Epoch 7/10\n",
      "3750/3750 [==============================] - 116s 31ms/step - loss: 0.0245 - accuracy: 0.9915 - val_loss: 0.5956 - val_accuracy: 0.8642\n",
      "Epoch 8/10\n",
      "3750/3750 [==============================] - 116s 31ms/step - loss: 0.0180 - accuracy: 0.9938 - val_loss: 0.5739 - val_accuracy: 0.8697\n",
      "Epoch 9/10\n",
      "3750/3750 [==============================] - 116s 31ms/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.6190 - val_accuracy: 0.8749\n",
      "Epoch 10/10\n",
      "3750/3750 [==============================] - 117s 31ms/step - loss: 0.0118 - accuracy: 0.9960 - val_loss: 0.7604 - val_accuracy: 0.8620\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "history = model.fit([en_sequences, my_sequences],\n",
    "                    np.array(train_label).astype(int), epochs=10, batch_size=32,\n",
    "                   validation_split=0.2, callbacks=model_callback)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
