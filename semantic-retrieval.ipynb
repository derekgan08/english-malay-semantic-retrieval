{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CPC353 Assignment 2**\n",
    "\n",
    "## **This assignment is to classifying English-Malay sentences according to their semantics**\n",
    "\n",
    "I used the GloVe 6B 100d pre-trained dwonloaded from [kaggle](https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt?resource=download) as the pre-trained embedding to embed the word vocabularies from the datasets given:\n",
    "\n",
    "\n",
    "1.   `train.en` - training files of English Sentences\n",
    "2.   `train.my` - training files of Malay Sentences\n",
    "3. `train.cl` - training files of the classes\n",
    "4. `test.en` - testing files of English Sentences\n",
    "5. `test.my` - testing files of Malay Sentences\n",
    "6. `test.cl` - testing files of the classes\n",
    "7. `dev.en` - validating files of English Sentences\n",
    "8. `dev.my` - validating files of Malay Sentences\n",
    "9. `dev.cl` - validating files of the classes\n",
    "\n",
    "There are 3 files for each training, testing and validation sets. The .en files are english sentences, the .my files are malay sentences, and the .cl files are the labels for english and malay files to indicate whether are both sentences similar. The .cl files are just 1s and 0s.\n",
    "\n",
    "The notebook is sectioned into multiple steps: Tokenizing text, embedding text, building neural network, and testing the network.\n",
    "\n",
    "### Neural network architecture\n",
    "The neural network architecture starts with two models. One for the English language, one for the Malay language. After that, I will concatenate both models into one layer where it will perform binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Reading the Text Files and Tokenizing the Texts**\n",
    "\n",
    "I will first read the files into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(text_path):\n",
    "    # This function will read the txt file and split the sentences with new line character, and will also remove the\n",
    "    # last character since it is an empty string\n",
    "    return open(text_path, \"r\", encoding=\"utf8\").read().split(\"\\n\")[:-1]\n",
    "\n",
    "train_text_en = read_text(\"/kaggle/input/english-malay/train.en\")\n",
    "train_text_my = read_text(\"/kaggle/input/english-malay/train.my\")\n",
    "train_label = read_text(\"/kaggle/input/english-malay/train.cl\")\n",
    "\n",
    "test_text_en = read_text(\"/kaggle/input/english-malay/test.en\")\n",
    "test_text_my = read_text(\"/kaggle/input/english-malay/test.my\")\n",
    "test_label = read_text(\"/kaggle/input/english-malay/test.cl\")\n",
    "\n",
    "dev_text_en = read_text(\"/kaggle/input/english-malay/dev.en\")\n",
    "dev_text_my = read_text(\"/kaggle/input/english-malay/dev.my\")\n",
    "dev_label = read_text(\"/kaggle/input/english-malay/dev.cl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining all imports for the project\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Input, Concatenate\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of text will be carried using the Tokenizer class from keras. Since I will be concatenating two models for two langauges. That means that I will have to tokenize both english and malay sentences separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text data\n",
    "en_tokenizer = Tokenizer()\n",
    "en_tokenizer.fit_on_texts(train_text_en)\n",
    "en_sequences = en_tokenizer.texts_to_sequences(train_text_en)\n",
    "\n",
    "my_tokenizer = Tokenizer()\n",
    "my_tokenizer.fit_on_texts(train_text_my)\n",
    "my_sequences = my_tokenizer.texts_to_sequences(train_text_my)\n",
    "\n",
    "# Find the maximum length between the two sequences, so we can pad them\n",
    "max_length = max(len(max(en_sequences, key=len)), len(max(my_sequences, key=len)))\n",
    "\n",
    "# Padding the sequences to same length\n",
    "en_sequences = pad_sequences(en_sequences, maxlen=max_length)\n",
    "my_sequences = pad_sequences(my_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in English text data:  39908\n",
      "Number of unique words in Malay text data:  23279\n"
     ]
    }
   ],
   "source": [
    "# Showing the number of vocabulary in both languages training data\n",
    "# Uncomment for the tokens and sequences\n",
    "\n",
    "print(\"Number of unique words in English text data: \", len(en_tokenizer.word_index))\n",
    "# print(\"English text data is: \", en_tokenizer.word_index)\n",
    "# print(\"English seqeunce: \", en_sequences)\n",
    "\n",
    "print(\"Number of unique words in Malay text data: \", len(my_tokenizer.word_index))\n",
    "# print(\"Malay text data is: \", my_tokenizer.word_index)\n",
    "# print(\"Malay seqeunce: \", my_sequences)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
